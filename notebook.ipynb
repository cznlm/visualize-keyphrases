{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyphrase Extraction & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/cznlm/visualize-keyphrases/blob/master/notebook.ipynb\" target =\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Keyphrase Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the pip install lines if necessary\n",
    "# !pip install rake_nltk\n",
    "# !pip install yake\n",
    "from rake_nltk import Rake\n",
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "r = Rake(min_length=1, max_length=4)\n",
    "y = yake.KeywordExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of sample sentences\n",
    "sentences = []\n",
    "sentences.append(\"Since the release of the first novel, Harry Potter and the Philosopher's Stone, on 26 June 1997, the books have found immense popularity, critical acclaim and commercial success worldwide.\")\n",
    "sentences.append(\"The Harry Potter novels are mainly directed at a young adult audience as opposed to an audience of middle grade readers, children, or adults.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:  Since the release of the first novel, Harry Potter and the Philosopher's Stone, on 26 June 1997, the books have found immense popularity, critical acclaim and commercial success worldwide.\n",
      "rake keyphrases: \n",
      "(9.0, 'found immense popularity')\n",
      "(9.0, 'commercial success worldwide')\n",
      "(9.0, '26 june 1997')\n",
      "(4.0, 'harry potter')\n",
      "(4.0, 'first novel')\n",
      "(4.0, 'critical acclaim')\n",
      "(1.0, 'stone')\n",
      "(1.0, 'since')\n",
      "(1.0, 'release')\n",
      "(1.0, 'philosopher')\n",
      "(1.0, 'books')\n",
      "yake keyphrases: \n",
      "(0.00601189725323029, 'found immense popularity')\n",
      "(0.00601189725323029, 'commercial success worldwide')\n",
      "(0.007237648981780116, 'harry potter')\n",
      "(0.007237648981780116, 'philosopher stone')\n",
      "(0.024805549895152507, 'immense popularity')\n",
      "(0.024805549895152507, 'critical acclaim')\n",
      "(0.024805549895152507, 'success worldwide')\n",
      "(0.04567049649971613, 'books have found')\n",
      "(0.04567049649971613, 'found immense')\n",
      "(0.04567049649971613, 'acclaim and commercial')\n",
      "(0.04567049649971613, 'commercial success')\n",
      "(0.057012387690331526, 'june')\n",
      "(0.07132737506917101, 'harry')\n",
      "(0.07132737506917101, 'stone')\n",
      "(0.10074171132118784, 'potter')\n",
      "(0.10074171132118784, 'philosopher')\n",
      "(0.12588232916135686, 'popularity')\n",
      "(0.12588232916135686, 'critical')\n",
      "(0.12588232916135686, 'worldwide')\n",
      "(0.19228376640925346, 'books')\n",
      "sentence:  The Harry Potter novels are mainly directed at a young adult audience as opposed to an audience of middle grade readers, children, or adults.\n",
      "rake keyphrases: \n",
      "(9.0, 'middle grade readers')\n",
      "(9.0, 'harry potter novels')\n",
      "(8.0, 'young adult audience')\n",
      "(4.0, 'mainly directed')\n",
      "(2.0, 'audience')\n",
      "(1.0, 'opposed')\n",
      "(1.0, 'children')\n",
      "(1.0, 'adults')\n",
      "yake keyphrases: \n",
      "(0.04013530368128323, 'harry potter')\n",
      "(0.05430009520648986, 'children')\n",
      "(0.07779073969035082, 'middle grade readers')\n",
      "(0.12239770796887664, 'grade readers')\n",
      "(0.1831603665399906, 'harry')\n",
      "(0.21067122094600266, 'potter')\n",
      "(0.24643857578759787, 'middle grade')\n",
      "(0.2640900048941729, 'readers')\n",
      "(0.3347019220635522, 'audience')\n",
      "(0.36241308132500155, 'young adult')\n",
      "(0.41292820610118347, 'grade')\n",
      "(0.447726487517707, 'young adult audience')\n",
      "(0.4788100520895411, 'directed')\n",
      "(0.4788100520895411, 'young')\n",
      "(0.4788100520895411, 'opposed')\n",
      "(0.4788100520895411, 'middle')\n",
      "(0.5555610639111797, 'adult')\n",
      "(0.5920897639832143, 'adult audience')\n",
      "(0.8102996859323182, 'audience as opposed')\n",
      "(0.8102996859323182, 'audience of middle')\n"
     ]
    }
   ],
   "source": [
    "# For each sentence, print the keyphrases that RAKE and YAKE extract\n",
    "for sentence in sentences:\n",
    "    print(\"sentence: \", sentence)\n",
    "    r.extract_keywords_from_text(sentence)\n",
    "    rake_keywords = r.get_ranked_phrases_with_scores()\n",
    "    print(\"rake keyphrases: \")\n",
    "    for kw in rake_keywords:\n",
    "        print(kw)\n",
    "    yake_keywords = y.extract_keywords(sentence)\n",
    "    print(\"yake keyphrases: \")\n",
    "    for kw in yake_keywords:\n",
    "        print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraphs:  [\"Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The novels chronicle the lives of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry. The main story arc concerns Harry's struggle against Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard governing body known as the Ministry of Magic and subjugate all wizards and Muggles (non-magical people).\", 'The central character in the series is Harry Potter, a boy who lives in the fictional town of Little Whinging, Surrey with his aunt, uncle, and cousin – the Dursleys – and discovers at the age of eleven that he is a wizard, though he lives in the ordinary world of non-magical people known as Muggles. The wizarding world exists parallel to the Muggle world, albeit hidden and in secrecy. His magical ability is inborn, and children with such abilities are invited to attend exclusive magic schools that teach the necessary skills to succeed in the wizarding world.']\n"
     ]
    }
   ],
   "source": [
    "# Create a list of sample paragraphs\n",
    "paragraphs = []\n",
    "paragraphs.append(\"Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The novels chronicle the lives of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry. The main story arc concerns Harry's struggle against Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard governing body known as the Ministry of Magic and subjugate all wizards and Muggles (non-magical people).\")\n",
    "paragraphs.append(\"The central character in the series is Harry Potter, a boy who lives in the fictional town of Little Whinging, Surrey with his aunt, uncle, and cousin – the Dursleys – and discovers at the age of eleven that he is a wizard, though he lives in the ordinary world of non-magical people known as Muggles. The wizarding world exists parallel to the Muggle world, albeit hidden and in secrecy. His magical ability is inborn, and children with such abilities are invited to attend exclusive magic schools that teach the necessary skills to succeed in the wizarding world.\")\n",
    "print(\"paragraphs: \", paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraph:  Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The novels chronicle the lives of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry. The main story arc concerns Harry's struggle against Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard governing body known as the Ministry of Magic and subjugate all wizards and Muggles (non-magical people).\n",
      "['become immortal', 'witchcraft and wizardry', 'ron weasley', 'novels chronicle', 'young wizard', 'british author', 'hermione granger', 'friends hermione granger', 'written by british', 'magical people ).', 'british author j', 'granger and ron', 'series of fantasy', 'fantasy novels written', 'rowling', 'dark wizard', 'lord voldemort', 'wizard governing body known', 'harry potter', 'british', 'hogwarts school', 'author', 'harry', 'ministry of magic', 'wizard', 'school of witchcraft', 'potter']\n",
      "paragraph:  The central character in the series is Harry Potter, a boy who lives in the fictional town of Little Whinging, Surrey with his aunt, uncle, and cousin – the Dursleys – and discovers at the age of eleven that he is a wizard, though he lives in the ordinary world of non-magical people known as Muggles. The wizarding world exists parallel to the Muggle world, albeit hidden and in secrecy. His magical ability is inborn, and children with such abilities are invited to attend exclusive magic schools that teach the necessary skills to succeed in the wizarding world.\n",
      "['world exists parallel', 'age of eleven', 'series is harry', 'cousin –', 'dursleys', 'muggle world', 'whinging', 'world', 'dursleys –', 'magical ability', 'ordinary world', 'boy who lives', 'little whinging', 'uncle', 'necessary skills', 'central character', 'wizarding world exists parallel', 'harry potter', 'magical people known', 'wizarding world exists', 'harry', 'albeit hidden', 'fictional town', 'attend exclusive magic schools', 'lives', 'non-magical people', 'potter', 'surrey', 'wizarding world']\n"
     ]
    }
   ],
   "source": [
    "# Define a function which takes in text and returns a list of keyphrases\n",
    "def get_keyphrases(text):\n",
    "    # yake has some trouble dealing with empty strings\n",
    "    if text.strip() == \"\":\n",
    "        return [\"\"]\n",
    "    keyphrases = []\n",
    "    r.extract_keywords_from_text(text)\n",
    "    rake_keywords = r.get_ranked_phrases_with_scores()\n",
    "    yake_keywords = y.extract_keywords(text)\n",
    "    for kw in rake_keywords:\n",
    "        if kw[0] > 3:\n",
    "            keyphrases.append(kw[1])\n",
    "    for kw in yake_keywords:\n",
    "        if kw[0] < 0.4:\n",
    "            keyphrases.append(kw[1])\n",
    "    return list(set(keyphrases)) # to remove duplicates\n",
    "\n",
    "# Test the function on our paragraphs\n",
    "for paragraph in paragraphs:\n",
    "    print(\"paragraph: \", paragraph)\n",
    "    print(get_keyphrases(paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import kmapper as km\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE ENTRY:  From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SHAPE:  (11314,)\n",
      "SAMPLE CATEGORY:  7\n",
      "SAMPLE CATEGORY NAME:  rec.autos\n"
     ]
    }
   ],
   "source": [
    "# Get the 20newsgroup dataset\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "raw_data = newsgroups.data\n",
    "X, category, category_names = np.array(raw_data), np.array(newsgroups.target), np.array(newsgroups.target_names)\n",
    "print(\"SAMPLE ENTRY: \", X[0])\n",
    "print(\"SHAPE: \", X.shape)\n",
    "print(\"SAMPLE CATEGORY: \", category[0])\n",
    "print(\"SAMPLE CATEGORY NAME: \", category_names[category[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE EXTRACTED PHRASES:  ['model name', 'day', 'please e mail', 'early 70s', 'early', 'funky looking car', 'could enlighten', 'bricklin', 'il brought', 'whatever info', 'enlighten', '2 door sports car', 'car', 'engine specs', 'wondering', 'neighborhood lerxst', 'called a bricklin', 'late 60s', 'front bumper', 'really small']\n",
      "SAMPLE ORIGINAL SENTENCE:     I was wondering if anyone out there could enlighten me on this car I saw the other day. It was a 2 door sports car, looked to be from the late 60s/ early 70s. It was called a Bricklin. The doors were really small. In addition, the front bumper was separate from the rest of the body. This is  all I know. If anyone can tellme a model name, engine specs, years of production, where this car is made, history, or whatever info you have on this funky looking car, please e mail.  Thanks,   IL         brought to you by your neighborhood Lerxst          \n",
      "SAMPLE JOINED SENTENCE:  model name day please e mail early 70s early funky looking car could enlighten bricklin il brought whatever info enlighten 2 door sports car car engine specs wondering neighborhood lerxst called a bricklin late 60s front bumper really small\n"
     ]
    }
   ],
   "source": [
    "# Format data -- get rid of email headers, new lines and tabs, and then create joined \"sentences\" for each sentence\n",
    "import re\n",
    "headers = [\"Lines: \", \"NNTP-Posting-Host: \", \"NNTP Posting Host: \"]\n",
    "data_formatted = []\n",
    "for entry in raw_data[:1000]:\n",
    "    occurrences = []\n",
    "    for header in headers:\n",
    "        occurrences.append(entry.lower().find(header.lower()))\n",
    "    champ = max(occurrences)\n",
    "    if champ != -1:\n",
    "        start = entry.find(\"\\n\", champ)\n",
    "        data_formatted.append(entry[start:])\n",
    "    else:\n",
    "        data_formatted.append(entry)\n",
    "data_formatted = [re.sub(\"[\\n\\t-]\", \" \", entry) for entry in data_formatted]\n",
    "extracted_phrases = [get_keyphrases(entry) for entry in data_formatted]\n",
    "print(\"SAMPLE EXTRACTED PHRASES: \", extracted_phrases[0])\n",
    "extracted_phrases_joined = [\" \".join(phrase) for phrase in extracted_phrases]\n",
    "print(\"SAMPLE ORIGINAL SENTENCE: \", data_formatted[0])\n",
    "print(\"SAMPLE JOINED SENTENCE: \", extracted_phrases_joined[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeplerMapper(verbose=2)\n",
      "..Composing projection pipeline of length 3:\n",
      "\tProjections: TfidfVectorizer(analyzer='char', max_df=0.83, min_df=0.05, ngram_range=(1, 6))\n",
      "\t\tTruncatedSVD(n_components=100, random_state=1729)\n",
      "\t\tIsomap(n_jobs=-1)\n",
      "\tDistance matrices: False\n",
      "False\n",
      "False\n",
      "\tScalers: None\n",
      "None\n",
      "MinMaxScaler()\n",
      "..Projecting on data shaped (1000,)\n",
      "\n",
      "..Projecting data using: \n",
      "\tTfidfVectorizer(analyzer='char', max_df=0.83, min_df=0.05, ngram_range=(1, 6))\n",
      "\n",
      "\n",
      "..Created projection shaped (1000, 5256)\n",
      "..Projecting on data shaped (1000, 5256)\n",
      "\n",
      "..Projecting data using: \n",
      "\tTruncatedSVD(n_components=100, random_state=1729)\n",
      "\n",
      "..Projecting on data shaped (1000, 100)\n",
      "\n",
      "..Projecting data using: \n",
      "\tIsomap(n_jobs=-1)\n",
      "\n",
      "\n",
      "..Scaling with: MinMaxScaler()\n",
      "\n",
      "SHAPE (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Project all of the \"sentences\" down into a 2D representation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mapper = km.KeplerMapper(verbose=2)\n",
    "\n",
    "projected_X = mapper.fit_transform(np.array(extracted_phrases_joined),\n",
    "    projection=[TfidfVectorizer(analyzer=\"char\",\n",
    "                                ngram_range=(1,6),\n",
    "                                max_df=0.83,\n",
    "                                min_df=0.05),\n",
    "                TruncatedSVD(n_components=100,\n",
    "                             random_state=1729),\n",
    "                Isomap(n_components=2,\n",
    "                       n_jobs=-1)],\n",
    "    scaler=[None, None, MinMaxScaler()])\n",
    "\n",
    "print(\"SHAPE\",projected_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping on data shaped (1000, 2) using lens shaped (1000, 2)\n",
      "\n",
      "Minimal points in hypercube before clustering: 3\n",
      "Creating 100 hypercubes.\n",
      "   > Found 1 clusters in hypercube 0.\n",
      "Cube_1 is empty.\n",
      "\n",
      "   > Found 1 clusters in hypercube 2.\n",
      "   > Found 1 clusters in hypercube 3.\n",
      "   > Found 1 clusters in hypercube 4.\n",
      "   > Found 1 clusters in hypercube 5.\n",
      "   > Found 1 clusters in hypercube 6.\n",
      "   > Found 1 clusters in hypercube 7.\n",
      "   > Found 1 clusters in hypercube 8.\n",
      "   > Found 1 clusters in hypercube 9.\n",
      "   > Found 1 clusters in hypercube 10.\n",
      "   > Found 1 clusters in hypercube 11.\n",
      "   > Found 1 clusters in hypercube 12.\n",
      "   > Found 1 clusters in hypercube 13.\n",
      "   > Found 1 clusters in hypercube 14.\n",
      "   > Found 1 clusters in hypercube 15.\n",
      "   > Found 1 clusters in hypercube 16.\n",
      "   > Found 1 clusters in hypercube 17.\n",
      "   > Found 1 clusters in hypercube 18.\n",
      "Cube_19 is empty.\n",
      "\n",
      "Cube_20 is empty.\n",
      "\n",
      "   > Found 1 clusters in hypercube 21.\n",
      "   > Found 1 clusters in hypercube 22.\n",
      "   > Found 1 clusters in hypercube 23.\n",
      "   > Found 1 clusters in hypercube 24.\n",
      "   > Found 1 clusters in hypercube 25.\n",
      "   > Found 1 clusters in hypercube 26.\n",
      "   > Found 1 clusters in hypercube 27.\n",
      "   > Found 1 clusters in hypercube 28.\n",
      "   > Found 1 clusters in hypercube 29.\n",
      "   > Found 1 clusters in hypercube 30.\n",
      "   > Found 1 clusters in hypercube 31.\n",
      "   > Found 1 clusters in hypercube 32.\n",
      "Cube_33 is empty.\n",
      "\n",
      "Cube_34 is empty.\n",
      "\n",
      "   > Found 1 clusters in hypercube 35.\n",
      "   > Found 1 clusters in hypercube 36.\n",
      "   > Found 1 clusters in hypercube 37.\n",
      "   > Found 1 clusters in hypercube 38.\n",
      "   > Found 1 clusters in hypercube 39.\n",
      "   > Found 1 clusters in hypercube 40.\n",
      "   > Found 1 clusters in hypercube 41.\n",
      "Cube_42 is empty.\n",
      "\n",
      "Cube_43 is empty.\n",
      "\n",
      "   > Found 1 clusters in hypercube 44.\n",
      "   > Found 1 clusters in hypercube 45.\n",
      "   > Found 1 clusters in hypercube 46.\n",
      "   > Found 1 clusters in hypercube 47.\n",
      "   > Found 1 clusters in hypercube 48.\n",
      "   > Found 1 clusters in hypercube 49.\n",
      "Cube_50 is empty.\n",
      "\n",
      "Cube_51 is empty.\n",
      "\n",
      "Cube_52 is empty.\n",
      "\n",
      "   > Found 1 clusters in hypercube 53.\n",
      "Cube_54 is empty.\n",
      "\n",
      "Cube_55 is empty.\n",
      "\n",
      "   > Found 1 clusters in hypercube 56.\n",
      "Cube_57 is empty.\n",
      "\n",
      "Cube_58 is empty.\n",
      "\n",
      "Cube_59 is empty.\n",
      "\n",
      "\n",
      "Created 71 edges and 45 nodes in 0:00:00.054530.\n"
     ]
    }
   ],
   "source": [
    "# Cluster the projected data\n",
    "from sklearn import cluster\n",
    "graph = mapper.map(projected_X, clusterer=cluster.DBSCAN(eps=0.5, min_samples=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE (11314, 947)\n",
      "FEATURE NAMES SAMPLE ['00', '000', '10', '100', '11', '12', '13', '14', '15', '16', '17', '18', '19', '1992', '1993', '1993apr15', '20', '200', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '408', '41', '42', '43', '44', '45', '49', '50', '500', '60', '70', '80', '90', '92', '93', 'able', 'ac', 'ac uk', 'accept', 'access', 'according', 'acs', 'act', 'action', 'actually', 'add', 'address', 'advance', 'advice', 'ago', 'agree', 'air', 'al', 'allow', 'allowed', 'america', 'american', 'andrew', 'answer', 'anti', 'anybody', 'apparently', 'appears', 'apple', 'application', 'apply', 'appreciate', 'appreciated', 'apr', 'apr 1993', 'apr 93', 'april', 'area', 'aren', 'argument', 'article', 'article 1993apr15', 'ask', 'asked', 'asking', 'assume', 'att', 'att com', 'au', 'available', 'average', 'avoid', 'away', 'bad', 'base', 'baseball', 'based', 'basic', 'basically', 'basis', 'bbs', 'believe', 'best', 'better', 'bible', 'big', 'bike', 'bit', 'bitnet', 'black', 'blue', 'board', 'bob', 'body', 'book', 'books', 'bought', 'box', 'break', 'brian', 'bring', 'brought', 'btw', 'build', 'building', 'built', 'bus', 'business', 'buy', 'ca', 'ca lines', 'california', 'called', 'came', 'canada', 'car', 'card', 'cards', 'care', 'carry', 'cars', 'case', 'cases', 'cause', 'cc', 'center', 'certain', 'certainly', 'chance', 'change', 'changed', 'cheap', 'check', 'chicago', 'children', 'chip', 'choice', 'chris', 'christ', 'christian', 'christians', 'church', 'city', 'claim', 'claims', 'class', 'clear', 'clearly', 'cleveland', 'clinton', 'clipper', 'close', 'cmu', 'cmu edu', 'code', 'college', 'color', 'colorado', 'com', 'com organization', 'com writes', 'come', 'comes', 'coming', 'comment', 'comments', 'common', 'communications', 'comp', 'company', 'complete', 'completely', 'computer', 'computer science', 'computing', 'condition', 'consider', 'considered', 'contact', 'continue', 'control', 'copy', 'corp', 'corporation', 'correct', 'cost', 'couldn', 'country', 'couple', 'course', 'court', 'cover', 'create', 'created', 'crime', 'cs', 'cso', 'cso uiuc', 'cso uiuc edu', 'cup', 'current', 'currently', 'cut', 'cwru', 'cwru edu', 'data', 'date', 'dave', 'david', 'day', 'days', 'dead', 'deal', 'death', 'decided', 'defense', 'deleted', 'department', 'dept', 'design', 'designed', 'details', 'development', 'device', 'did', 'didn', 'die', 'difference', 'different', 'difficult', 'directly', 'disclaimer', 'discussion', 'disk', 'display', 'distribution', 'distribution na', 'distribution na lines', 'distribution usa', 'distribution usa lines', 'distribution world', 'distribution world nntp', 'distribution world organization', 'division', 'dod', 'does', 'does know', 'doesn', 'doing', 'don', 'don know', 'don think', 'don want', 'dos', 'doubt', 'dr', 'drive', 'driver', 'drivers', 'early', 'earth', 'easily', 'east', 'easy', 'ed', 'edu', 'edu article', 'edu au', 'edu david', 'edu organization', 'edu organization university', 'edu reply', 'edu subject', 'edu writes', 'effect', 'email', 'encryption', 'end', 'engineering', 'entire', 'error', 'especially', 'evidence', 'exactly', 'example', 'excellent', 'exist', 'exists', 'expect', 'experience', 'explain', 'expressed', 'extra', 'face', 'fact', 'faith', 'family', 'fan', 'faq', 'far', 'fast', 'faster', 'fax', 'federal', 'feel', 'figure', 'file', 'files', 'final', 'finally', 'fine', 'folks', 'follow', 'following', 'force', 'forget', 'form', 'frank', 'free', 'friend', 'ftp', 'future', 'game', 'games', 'gave', 'general', 'generally', 'germany', 'gets', 'getting', 'given', 'gives', 'giving', 'gmt', 'god', 'goes', 'going', 'gone', 'good', 'got', 'gov', 'government', 'graphics', 'great', 'greatly', 'ground', 'group', 'groups', 'guess', 'gun', 'guns', 'guy', 'half', 'hand', 'happen', 'happened', 'happens', 'happy', 'hard', 'hardware', 'haven', 'having', 'head', 'hear', 'heard', 'heart', 'hell']\n"
     ]
    }
   ],
   "source": [
    "# Get counts of phrases from data (to be used for the label and for cluster statistics)\n",
    "vec = TfidfVectorizer(analyzer=\"word\",\n",
    "                      strip_accents=\"unicode\",\n",
    "                      stop_words=\"english\",\n",
    "                      ngram_range=(1,3),\n",
    "                      max_df=0.97,\n",
    "                      min_df=0.02)\n",
    "\n",
    "interpretable_inverse_X = vec.fit_transform(X).toarray()\n",
    "interpretable_inverse_X_names = vec.get_feature_names()\n",
    "\n",
    "print(\"SHAPE\", interpretable_inverse_X.shape)\n",
    "print(\"FEATURE NAMES SAMPLE\", interpretable_inverse_X_names[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote visualization to: newsgroups20.html\n"
     ]
    }
   ],
   "source": [
    "# Visualize data!\n",
    "html = mapper.visualize(graph,\n",
    "                        X=interpretable_inverse_X,\n",
    "                        X_names=interpretable_inverse_X_names,\n",
    "                        path_html=\"newsgroups20.html\",\n",
    "                        lens=projected_X,\n",
    "                        lens_names=[\"ISOMAP1\", \"ISOMAP2\"],\n",
    "                        title=\"Newsgroups20: Latent Semantic Char-gram Analysis with Isometric Embedding\",\n",
    "                        custom_tooltips=np.array([category_names[ys] for ys in category]),\n",
    "                        color_function=category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "        .container { width:100% !important; }\n",
       "        .output_scroll {height: 800px !important;}\n",
       "        </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cheyennezhang/Documents/NLMatics/nlm-ingestor-v2/nlm-env/lib/python3.8/site-packages/IPython/core/display.py:717: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe src=newsgroups20.html width=100%% height=800 frameBorder=\"0\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Open visualization here in the notebook\n",
    "from kmapper import jupyter\n",
    "jupyter.display(path_html=\"newsgroups20.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('nlm-env': venv)",
   "language": "python",
   "name": "python38164bitnlmenvvenvad24dd00e2184e1ca089866e90311227"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
