{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyphrase Extraction & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Keyphrase Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if necessary\n",
    "# !pip install rake_nltk\n",
    "# !pip install yake\n",
    "from rake_nltk import Rake\n",
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Rake(min_length=1, max_length=4)\n",
    "y = yake.KeywordExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "sentences.append(\"Since the release of the first novel, Harry Potter and the Philosopher's Stone, on 26 June 1997, the books have found immense popularity, critical acclaim and commercial success worldwide.\")\n",
    "sentences.append(\"The Harry Potter novels are mainly directed at a young adult audience as opposed to an audience of middle grade readers, children, or adults.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    print(\"sentence: \", sentence)\n",
    "    r.extract_keywords_from_text(sentence)\n",
    "    rake_keywords = r.get_ranked_phrases_with_scores()\n",
    "    print(\"rake keyphrases: \")\n",
    "    for kw in rake_keywords:\n",
    "        print(kw)\n",
    "    yake_keywords = y.extract_keywords(sentence)\n",
    "    print(\"yake keyphrases: \")\n",
    "    for kw in yake_keywords:\n",
    "        print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "paragraphs.append(\"Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The novels chronicle the lives of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry. The main story arc concerns Harry's struggle against Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard governing body known as the Ministry of Magic and subjugate all wizards and Muggles (non-magical people).\")\n",
    "paragraphs.append(\"The central character in the series is Harry Potter, a boy who lives in the fictional town of Little Whinging, Surrey with his aunt, uncle, and cousin – the Dursleys – and discovers at the age of eleven that he is a wizard, though he lives in the ordinary world of non-magical people known as Muggles. The wizarding world exists parallel to the Muggle world, albeit hidden and in secrecy. His magical ability is inborn, and children with such abilities are invited to attend exclusive magic schools that teach the necessary skills to succeed in the wizarding world.\")\n",
    "print(\"paragraphs: \", paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyphrases(text):\n",
    "    if text.strip() == \"\":\n",
    "        return [\"\"]\n",
    "    keyphrases = []\n",
    "    r.extract_keywords_from_text(text)\n",
    "    rake_keywords = r.get_ranked_phrases_with_scores()\n",
    "    yake_keywords = y.extract_keywords(text)\n",
    "    for kw in rake_keywords:\n",
    "        if kw[0] > 3:\n",
    "            keyphrases.append(kw[1])\n",
    "    for kw in yake_keywords:\n",
    "        if kw[0] < 0.4:\n",
    "            keyphrases.append(kw[1])\n",
    "    return list(set(keyphrases)) # to remove duplicates\n",
    "\n",
    "for paragraph in paragraphs:\n",
    "    print(\"paragraph: \", paragraph)\n",
    "    print(get_keyphrases(paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import kmapper as km\n",
    "import numpy as np\n",
    "# !pip install scikit-learn\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "raw_data = newsgroups.data\n",
    "X, target, target_names = np.array(raw_data), np.array(newsgroups.target), np.array(newsgroups.target_names)\n",
    "print(\"SAMPLE: \", X[0])\n",
    "print(\"SHAPE: \", X.shape)\n",
    "print(\"TARGET: \", target_names[target[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data\n",
    "import re\n",
    "# get rid of headers\n",
    "headers = [\"Lines: \", \"NNTP-Posting-Host: \", \"NNTP Posting Host: \"]\n",
    "data_formatted = []\n",
    "for entry in raw_data[:1000]:\n",
    "    occurrences = []\n",
    "    for header in headers:\n",
    "        occurrences.append(entry.lower().find(header.lower()))\n",
    "    champ = max(occurrences)\n",
    "    if champ != -1:\n",
    "        start = entry.find(\"\\n\", champ)\n",
    "        data_formatted.append(entry[start:])\n",
    "    else:\n",
    "        data_formatted.append(entry)\n",
    "data_formatted = [re.sub(\"[\\n\\t-]\", \" \", entry) for entry in data_formatted]\n",
    "extracted_phrases = [get_keyphrases(entry) for entry in data_formatted]\n",
    "print(\"SAMPLE EXTRACTED PHRASES\", extracted_phrases[0])\n",
    "extracted_phrases_joined = [\" \".join(phrase) for phrase in extracted_phrases]\n",
    "print(\"ORIGINAL SENTENCE\", data_formatted[0])\n",
    "print(\"JOINED SENTENCE\", extracted_phrases_joined[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mapper = km.KeplerMapper(verbose=2)\n",
    "\n",
    "projected_X = mapper.fit_transform(np.array(extracted_phrases_joined),\n",
    "    projection=[TfidfVectorizer(analyzer=\"char\",\n",
    "                                ngram_range=(1,6),\n",
    "                                max_df=0.83,\n",
    "                                min_df=0.05),\n",
    "                TruncatedSVD(n_components=100,\n",
    "                             random_state=1729),\n",
    "                Isomap(n_components=2,\n",
    "                       n_jobs=-1)],\n",
    "    scaler=[None, None, MinMaxScaler()])\n",
    "\n",
    "print(\"SHAPE\",projected_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster data\n",
    "from sklearn import cluster\n",
    "graph = mapper.map(projected_X, clusterer=cluster.DBSCAN(eps=0.5, min_samples=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get features from data\n",
    "vec = TfidfVectorizer(analyzer=\"word\",\n",
    "                      strip_accents=\"unicode\",\n",
    "                      stop_words=\"english\",\n",
    "                      ngram_range=(1,3),\n",
    "                      max_df=0.97,\n",
    "                      min_df=0.02)\n",
    "\n",
    "interpretable_inverse_X = vec.fit_transform(X).toarray()\n",
    "interpretable_inverse_X_names = vec.get_feature_names()\n",
    "\n",
    "print(\"SHAPE\", interpretable_inverse_X.shape)\n",
    "print(\"FEATURE NAMES SAMPLE\", interpretable_inverse_X_names[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data!\n",
    "html = mapper.visualize(graph,\n",
    "                        X=interpretable_inverse_X,\n",
    "                        X_names=interpretable_inverse_X_names,\n",
    "                        path_html=\"newsgroups20.html\",\n",
    "                        lens=projected_X,\n",
    "                        lens_names=[\"ISOMAP1\", \"ISOMAP2\"],\n",
    "                        title=\"Newsgroups20: Latent Semantic Char-gram Analysis with Isometric Embedding\",\n",
    "                        custom_tooltips=np.array([target_names[ys] for ys in target]),\n",
    "                        color_function=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
